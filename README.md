# Projeto de PortfÃ³lio: Pipeline de Dados e Sala de SituaÃ§Ã£o da Dengue no Brasil

## ðŸŽ¯ Objetivo

Este projeto implementa um pipeline de dados ponta a ponta para processar microdados de notificaÃ§Ãµes de casos de Dengue no Brasil, disponibilizados publicamente pelo DATASUS (SINAN). O objetivo Ã© transformar dados brutos e massivos em um dashboard analÃ­tico e interativo ("Sala de SituaÃ§Ã£o") que permita o monitoramento epidemiolÃ³gico da doenÃ§a em nÃ­vel nacional.

O projeto aborda desafios do mundo real, como o manuseio de grandes volumes de dados (milhÃµes de registros), a limpeza de dados governamentais e a implementaÃ§Ã£o de uma arquitetura de dados baseada em nuvem.

## ðŸ›ï¸ Arquitetura do Pipeline

Para simular um ambiente de dados profissional, foi utilizada a seguinte arquitetura:


1.  **Data Lake (Armazenamento Bruto):** Os arquivos CSV originais do DATASUS sÃ£o armazenados em um bucket no **Google Cloud Storage**, servindo como nosso repositÃ³rio central de dados brutos.
2.  **Processamento (ETL):** Um script Python local orquestra todo o processo. Ele se conecta de forma segura ao Google Cloud, baixa os dados em memÃ³ria e utiliza a biblioteca **Polars** para processamento de alta performance.
3.  **Data Warehouse (Armazenamento AnalÃ­tico):** ApÃ³s a limpeza, filtragem e agregaÃ§Ã£o dos dados (casos confirmados por municÃ­pio e mÃªs), o resultado consolidado Ã© carregado em um banco de dados **PostgreSQL**, servindo como nossa fonte de verdade para anÃ¡lises.
4.  **VisualizaÃ§Ã£o (BI):** O dashboard final Ã© construÃ­do no **Google Looker Studio**, que se conecta aos dados preparados para gerar mapas, grÃ¡ficos e filtros interativos.

## ðŸš€ Tecnologias Utilizadas

- **Nuvem:** Google Cloud Platform (GCP)
  - **Data Lake:** Google Cloud Storage
- **Linguagem:** Python
- **Bibliotecas de Processamento:**
  - **Polars:** Para manipulaÃ§Ã£o de grandes volumes de dados com alta performance e baixo consumo de memÃ³ria.
  - **Pandas:** Para enriquecimento dos dados e integraÃ§Ã£o com outras ferramentas.
- **Banco de Dados:** PostgreSQL
- **Ferramenta de BI:** Google Looker Studio
- **AutenticaÃ§Ã£o:** Google Cloud Service Accounts

## ðŸ“Š Dashboard - Sala de SituaÃ§Ã£o

O dashboard interativo permite a anÃ¡lise dos dados sob diversas perspectivas, incluindo:
-   Um mapa geoespacial do Brasil que mostra a concentraÃ§Ã£o de casos por estado.
-   Uma sÃ©rie temporal para identificar picos sazonais da doenÃ§a.
-   Filtros interativos para explorar os dados por perÃ­odo e regiÃ£o.

**[>> Clique aqui para ver o dashboard interativo <<](https://lookerstudio.google.com/reporting/50adc11a-611b-452e-9957-15443d9927d1)**

## âš™ï¸ Como Executar o Projeto

Para replicar este ambiente e executar o pipeline, siga os passos abaixo:

### 1. PrÃ©-requisitos
- Ter o [Python 3](https://www.python.org/downloads/) instalado.
- Ter o [PostgreSQL](https://www.postgresql.org/download/) instalado. O **pgAdmin**, que Ã© a interface grÃ¡fica, geralmente Ã© incluÃ­do na instalaÃ§Ã£o.
- Uma conta no [Google Cloud Platform](https://console.cloud.google.com/).

### 2. ConfiguraÃ§Ã£o do Ambiente

1.  **Adicione os Dados Brutos:** [https://opendatasus.saude.gov.br/gl/dataset/arboviroses-dengue](https://opendatasus.saude.gov.br/gl/dataset/arboviroses-dengue)

2.  **Clone o RepositÃ³rio:**
    ```bash
    git clone https://github.com/AndreVVoigt/pipeline-dengue
    ```
3.  **Instale as DependÃªncias:**
    ```bash
    pip install polars google-cloud-storage pandas sqlalchemy psycopg2-binary python-dotenv pyarrow
    ```
4.  **Configure o Google Cloud:**
    -   Crie um projeto no GCP.
    -   Crie um bucket no Cloud Storage e faÃ§a o upload dos arquivos CSV brutos da Dengue.
    -   Crie uma Conta de ServiÃ§o (`Service Account`) com o papel de "Visualizador de Objetos do Storage".
    -   Gere uma chave JSON para esta conta, renomeie-a para `gcp_credentials.json` e coloque-a na raiz do projeto.
5.  **VariÃ¡veis de Ambiente:**
    -   Crie um arquivo `.env` na raiz do projeto.
    -   Adicione a seguinte linha, apontando para sua chave JSON:
        ```
        GOOGLE_APPLICATION_CREDENTIALS="gcp_credentials.json"
        ```
6.  **Configure o Banco de Dados:**
    -   Usando o pgAdmin, crie um banco de dados (ex: `dengue_db`).
    -   Execute o script do arquivo `schema.sql` (se vocÃª tiver um) para criar a tabela `fato_casos_dengue`.

### 3. ExecuÃ§Ã£o do Pipeline
1.  **Atualize as ConfiguraÃ§Ãµes:** No script `etl_dengue.py`, atualize as variÃ¡veis `BUCKET_NAME` e as credenciais do seu banco de dados.
2.  **Execute o ETL:**
    ```bash
    python etl_dengue.py
    ```
3.  **Prepare os Dados para o Dashboard:**
    -   Atualize as credenciais do banco no script `preparar_dashboard.py`.
    -   Execute o script para gerar o CSV final enriquecido:
        ```bash
        python preparar_dashboard.py
        ```
4.  **Visualize:** Use o arquivo `dados_dashboard_dengue.csv` gerado como fonte de dados para criar seu dashboard no Looker Studio.